{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8f1b1c1",
   "metadata": {},
   "source": [
    "# Excel File Analysis - Price Comparison Report\n",
    "\n",
    "This notebook reads and analyzes the Excel file from ComparisionData folder, specifically examining the \"detailed report\" sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cdc712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "PROCESSING: COMBINED FILE ONLY\n",
      "============================================================\n",
      "Processing folder: ComparisionData\n",
      "Found 7 files to process:\n",
      "  - Price comparision_03_06.xlsx\n",
      "  - Price comparision_04_06.xlsx\n",
      "  - Price comparision_06_06.xlsx\n",
      "  - Price comparision_10_06.xlsx\n",
      "  - Price comparision_11_06.xlsx\n",
      "  - Price comparision_12_06.xlsx\n",
      "  - Price comparision_13_06.xlsx\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_03_06.xlsx\n",
      "Extracted date from filename: 2025-06-03\n",
      "\n",
      "============================================================\n",
      "PROCESSING: COMBINED FILE ONLY\n",
      "============================================================\n",
      "Processing folder: ComparisionData\n",
      "Found 7 files to process:\n",
      "  - Price comparision_03_06.xlsx\n",
      "  - Price comparision_04_06.xlsx\n",
      "  - Price comparision_06_06.xlsx\n",
      "  - Price comparision_10_06.xlsx\n",
      "  - Price comparision_11_06.xlsx\n",
      "  - Price comparision_12_06.xlsx\n",
      "  - Price comparision_13_06.xlsx\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_03_06.xlsx\n",
      "Extracted date from filename: 2025-06-03\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (900, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_04_06.xlsx\n",
      "Extracted date from filename: 2025-06-04\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (900, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_04_06.xlsx\n",
      "Extracted date from filename: 2025-06-04\n",
      "Error processing Price comparision_04_06.xlsx: 'ORG_SELLER'\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_06_06.xlsx\n",
      "Extracted date from filename: 2025-06-06\n",
      "Error processing Price comparision_04_06.xlsx: 'ORG_SELLER'\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_06_06.xlsx\n",
      "Extracted date from filename: 2025-06-06\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (901, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_10_06.xlsx\n",
      "Extracted date from filename: 2025-06-10\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (901, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_10_06.xlsx\n",
      "Extracted date from filename: 2025-06-10\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (889, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_11_06.xlsx\n",
      "Extracted date from filename: 2025-06-11\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (889, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_11_06.xlsx\n",
      "Extracted date from filename: 2025-06-11\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (889, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_12_06.xlsx\n",
      "Extracted date from filename: 2025-06-12\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (889, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_12_06.xlsx\n",
      "Extracted date from filename: 2025-06-12\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (889, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_13_06.xlsx\n",
      "Extracted date from filename: 2025-06-13\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (889, 14)\n",
      "\n",
      "Processing file: ComparisionData\\Price comparision_13_06.xlsx\n",
      "Extracted date from filename: 2025-06-13\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (889, 14)\n",
      "\n",
      "Combining 6 pivot tables...\n",
      "Combined dataframe shape: (5357, 14)\n",
      "Found sellers: ['Amazon-Amazon', 'Carrefour-Carrefour', 'Dyson-dyson', 'Emax-emax', 'Jumbo-jumbo', 'Noon-noon', 'Sharaf DG-Sharaf DG', 'Sony-Sony']\n",
      "Pivot table created with shape: (889, 14)\n",
      "\n",
      "Combining 6 pivot tables...\n",
      "Combined dataframe shape: (5357, 14)\n",
      "\n",
      "Combined data saved to: ComparisionData\\Combined_Price_Analysis.xlsx\n",
      "Total rows: 5357\n",
      "Date range: 2025-06-03 to 2025-06-13\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "File created:\n",
      "✓ Combined_Price_Analysis.xlsx - All historical data\n",
      "\n",
      "Total rows processed: 5357\n",
      "Date range: 2025-06-03 to 2025-06-13\n",
      "\n",
      "Combined data saved to: ComparisionData\\Combined_Price_Analysis.xlsx\n",
      "Total rows: 5357\n",
      "Date range: 2025-06-03 to 2025-06-13\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "File created:\n",
      "✓ Combined_Price_Analysis.xlsx - All historical data\n",
      "\n",
      "Total rows processed: 5357\n",
      "Date range: 2025-06-03 to 2025-06-13\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def process_single_file(file_path, year):\n",
    "    \"\"\"\n",
    "    Process a single Price comparison file and return pivot table\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing file: {file_path}\")\n",
    "    \n",
    "    # Extract date from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    date_match = re.search(r'Price comparision_(\\d{2})_(\\d{2})\\.xlsx', filename)\n",
    "    \n",
    "    if date_match:\n",
    "        day = date_match.group(1)\n",
    "        month = date_match.group(2)\n",
    "        \n",
    "        # Create date string\n",
    "        date_str = f\"{year}-{month}-{day}\"\n",
    "        print(f\"Extracted date from filename: {date_str}\")\n",
    "        \n",
    "        # Convert to datetime for validation\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid date extracted from filename\")\n",
    "            formatted_date = f\"{year}-{month}-{day}\"\n",
    "    else:\n",
    "        print(\"Could not extract date from filename\")\n",
    "        formatted_date = f\"{year}-01-01\"  # Default date\n",
    "    \n",
    "    try:\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        \n",
    "        # Convert sheet names to lowercase and find target sheet\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        target_sheet = None\n",
    "        for sheet in sheet_names:\n",
    "            if sheet.lower() == \"detailed report\":\n",
    "                target_sheet = sheet\n",
    "                break\n",
    "        \n",
    "        if not target_sheet:\n",
    "            print(f\"No 'detailed report' sheet found in {filename}\")\n",
    "            return None\n",
    "        \n",
    "        # Load the target sheet\n",
    "        df = pd.read_excel(file_path, sheet_name=target_sheet)\n",
    "        \n",
    "        # Case insensitive filtering function\n",
    "        def matches_pattern(value):\n",
    "            if pd.isna(value) or not isinstance(value, str):\n",
    "                return False\n",
    "            parts = value.split('-')\n",
    "            if len(parts) != 2:\n",
    "                return False\n",
    "            # Allow alphabetic characters and spaces, remove extra spaces for comparison\n",
    "            part1_clean = parts[0].strip()\n",
    "            part2_clean = parts[1].strip()\n",
    "            \n",
    "            # Both parts should contain at least some alphabetic characters\n",
    "            if not (any(c.isalpha() for c in part1_clean) and any(c.isalpha() for c in part2_clean)):\n",
    "                return False\n",
    "            \n",
    "            # Compare case insensitive and handle spaces\n",
    "            return part1_clean.lower() == part2_clean.lower()\n",
    "        \n",
    "        # Filter data\n",
    "        filtered_df = df[df['ORG_SELLER'].apply(matches_pattern)]\n",
    "        \n",
    "        if len(filtered_df) == 0:\n",
    "            print(f\"No matching data found in {filename}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract company names from ORG_SELLER for column headers\n",
    "        unique_sellers = sorted(filtered_df['ORG_SELLER'].unique())\n",
    "        print(f\"Found sellers: {unique_sellers}\")\n",
    "        \n",
    "        # Define priority order for companies\n",
    "        priority_order = ['Jumbo', 'Sharaf DG', 'Emax', 'Noon', 'Amazon', 'Carrefour', 'Dyson']\n",
    "        \n",
    "        # Create a base dataframe\n",
    "        pivot_base = filtered_df[['SKU', 'TITLE', 'CATEGORY', 'ORG_SELLER', 'OFFER PRICE']].copy()\n",
    "        \n",
    "        # Extract company name from ORG_SELLER\n",
    "        pivot_base['Company'] = pivot_base['ORG_SELLER'].str.split('-').str[0]\n",
    "        \n",
    "        # Create pivot table with OFFER PRICE as values\n",
    "        pivot_table = pivot_base.pivot_table(\n",
    "            index=['SKU', 'TITLE', 'CATEGORY'], \n",
    "            columns='Company', \n",
    "            values='OFFER PRICE', \n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Add Date column as the first column\n",
    "        pivot_table.insert(0, 'Date', formatted_date)\n",
    "        \n",
    "        # Reorder columns according to priority (after Date)\n",
    "        available_companies = [col for col in pivot_table.columns if col in priority_order]\n",
    "        other_companies = [col for col in pivot_table.columns if col not in priority_order and col not in ['Date', 'SKU', 'TITLE', 'CATEGORY']]\n",
    "        \n",
    "        # Create ordered column list starting with Date\n",
    "        ordered_columns = ['Date', 'SKU', 'TITLE', 'CATEGORY']\n",
    "        \n",
    "        # Add companies in priority order\n",
    "        for company in priority_order:\n",
    "            if company in pivot_table.columns:\n",
    "                ordered_columns.append(company)\n",
    "        \n",
    "        # Add any other companies at the end\n",
    "        for company in other_companies:\n",
    "            ordered_columns.append(company)\n",
    "        \n",
    "        # Reorder the pivot table\n",
    "        pivot_table = pivot_table[ordered_columns]\n",
    "        \n",
    "        # Create \"Jumbo Higher than\" and \"Jumbo Lower than\" columns\n",
    "        def get_jumbo_higher_than(row):\n",
    "            jumbo_price = row.get('Jumbo', None)\n",
    "            \n",
    "            if pd.isna(jumbo_price) or jumbo_price is None:\n",
    "                return \"\"\n",
    "            \n",
    "            higher_than = []\n",
    "            \n",
    "            # Check each company (excluding Jumbo)\n",
    "            for company in priority_order[1:]:  # Skip Jumbo\n",
    "                if company in row and not pd.isna(row[company]):\n",
    "                    company_price = row[company]\n",
    "                    # Add relaxation of 1: Jumbo must be more than company_price + 1 to be considered higher\n",
    "                    if jumbo_price > (company_price + 1):  # Jumbo is higher with relaxation\n",
    "                        higher_than.append(company)\n",
    "            \n",
    "            # Check other companies not in priority order\n",
    "            for company in other_companies:\n",
    "                if company in row and not pd.isna(row[company]):\n",
    "                    company_price = row[company]\n",
    "                    # Add relaxation of 1: Jumbo must be more than company_price + 1 to be considered higher\n",
    "                    if jumbo_price > (company_price + 1):  # Jumbo is higher with relaxation\n",
    "                        higher_than.append(company)\n",
    "            \n",
    "            return \", \".join(higher_than) if higher_than else \"\"\n",
    "        \n",
    "        def get_jumbo_lower_than(row):\n",
    "            jumbo_price = row.get('Jumbo', None)\n",
    "            \n",
    "            if pd.isna(jumbo_price) or jumbo_price is None:\n",
    "                return \"\"\n",
    "            \n",
    "            lower_than = []\n",
    "            \n",
    "            # Check each company (excluding Jumbo)\n",
    "            for company in priority_order[1:]:  # Skip Jumbo\n",
    "                if company in row and not pd.isna(row[company]):\n",
    "                    company_price = row[company]\n",
    "                    # Add relaxation of 1: Jumbo must be less than company_price - 1 to be considered lower\n",
    "                    if jumbo_price < (company_price - 1):  # Jumbo is lower with relaxation\n",
    "                        lower_than.append(company)\n",
    "            \n",
    "            # Check other companies not in priority order\n",
    "            for company in other_companies:\n",
    "                if company in row and not pd.isna(row[company]):\n",
    "                    company_price = row[company]\n",
    "                    # Add relaxation of 1: Jumbo must be less than company_price - 1 to be considered lower\n",
    "                    if jumbo_price < (company_price - 1):  # Jumbo is lower with relaxation\n",
    "                        lower_than.append(company)\n",
    "            \n",
    "            return \", \".join(lower_than) if lower_than else \"\"\n",
    "        \n",
    "        # Apply the functions to create the new columns\n",
    "        pivot_table['Jumbo Higher than'] = pivot_table.apply(get_jumbo_higher_than, axis=1)\n",
    "        pivot_table['Jumbo Lower than'] = pivot_table.apply(get_jumbo_lower_than, axis=1)\n",
    "        \n",
    "        print(f\"Pivot table created with shape: {pivot_table.shape}\")\n",
    "        return pivot_table\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_folder_and_combine(folder_path, year, output_filename=\"Combined_Price_Analysis.xlsx\", create_latest_file=True):\n",
    "    \"\"\"\n",
    "    Process all Price comparison files in a folder and combine them into one Excel file\n",
    "    Also creates a separate file with just the latest date's data\n",
    "    \"\"\"\n",
    "    print(f\"Processing folder: {folder_path}\")\n",
    "    \n",
    "    # Find all files matching the pattern\n",
    "    file_pattern = os.path.join(folder_path, \"Price comparision_*.xlsx\")\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found matching pattern in {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(files)} files to process:\")\n",
    "    for file in files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    # Process each file and collect pivot tables\n",
    "    all_pivot_tables = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        pivot_table = process_single_file(file_path, year)\n",
    "        if pivot_table is not None:\n",
    "            all_pivot_tables.append(pivot_table)\n",
    "    \n",
    "    if not all_pivot_tables:\n",
    "        print(\"No valid pivot tables were created\")\n",
    "        return\n",
    "    \n",
    "    # Combine all pivot tables\n",
    "    print(f\"\\nCombining {len(all_pivot_tables)} pivot tables...\")\n",
    "    combined_df = pd.concat(all_pivot_tables, ignore_index=True)\n",
    "    \n",
    "    # Sort by Date and SKU\n",
    "    combined_df = combined_df.sort_values(['Date', 'SKU'])\n",
    "    \n",
    "    print(f\"Combined dataframe shape: {combined_df.shape}\")\n",
    "    \n",
    "    # Save combined data to Excel file\n",
    "    output_path = os.path.join(folder_path, output_filename)\n",
    "    combined_df.to_excel(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\nCombined data saved to: {output_path}\")\n",
    "    print(f\"Total rows: {len(combined_df)}\")\n",
    "    print(f\"Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")\n",
    "    \n",
    "    # Create separate file for latest date data\n",
    "    if create_latest_file:\n",
    "        latest_date = combined_df['Date'].max()\n",
    "        latest_data = combined_df[combined_df['Date'] == latest_date].copy()\n",
    "        \n",
    "        if not latest_data.empty:\n",
    "            # Create filename for latest date file\n",
    "            latest_filename = f\"Latest_Date_Analysis_{latest_date}.xlsx\"\n",
    "            latest_output_path = os.path.join(folder_path, latest_filename)\n",
    "            \n",
    "            # Sort latest data by SKU for better readability\n",
    "            latest_data = latest_data.sort_values(['SKU', 'TITLE'])\n",
    "            \n",
    "            # Save latest date data\n",
    "            latest_data.to_excel(latest_output_path, index=False)\n",
    "            \n",
    "            print(f\"\\nLatest date data saved to: {latest_output_path}\")\n",
    "            print(f\"Latest date: {latest_date}\")\n",
    "            print(f\"Latest date rows: {len(latest_data)}\")\n",
    "            print(f\"Unique SKUs in latest date: {latest_data['SKU'].nunique()}\")\n",
    "            \n",
    "            # Show summary of latest data\n",
    "            companies_in_latest = [col for col in latest_data.columns if col in ['Jumbo', 'Sharaf DG', 'Emax', 'Noon', 'Amazon', 'Carrefour', 'Dyson', 'Sony']]\n",
    "            print(f\"Companies with data in latest date: {companies_in_latest}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nNo latest date data found\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def process_latest_file_only(folder_path, year):\n",
    "    \"\"\"\n",
    "    Process only the latest file in the folder for faster execution\n",
    "    \"\"\"\n",
    "    print(f\"Finding latest file in: {folder_path}\")\n",
    "    \n",
    "    # Find all files matching the pattern\n",
    "    file_pattern = os.path.join(folder_path, \"Price comparision_*.xlsx\")\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"No files found matching pattern in {folder_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Find the latest file based on date in filename\n",
    "    latest_file = None\n",
    "    latest_date = None\n",
    "    \n",
    "    for file_path in files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        date_match = re.search(r'Price comparision_(\\d{2})_(\\d{2})\\.xlsx', filename)\n",
    "        \n",
    "        if date_match:\n",
    "            day = date_match.group(1)\n",
    "            month = date_match.group(2)\n",
    "            try:\n",
    "                # Create date for comparison\n",
    "                file_date = datetime.strptime(f\"{year}-{month}-{day}\", \"%Y-%m-%d\")\n",
    "                if latest_date is None or file_date > latest_date:\n",
    "                    latest_date = file_date\n",
    "                    latest_file = file_path\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    if not latest_file:\n",
    "        print(\"No valid date files found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Processing latest file: {os.path.basename(latest_file)}\")\n",
    "    print(f\"Latest date: {latest_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Process the latest file\n",
    "    pivot_table = process_single_file(latest_file, year)\n",
    "    \n",
    "    if pivot_table is not None:\n",
    "        # Save latest file data\n",
    "        latest_filename = f\"Latest_Date_Analysis_{latest_date.strftime('%Y-%m-%d')}.xlsx\"\n",
    "        output_path = os.path.join(folder_path, latest_filename)\n",
    "        \n",
    "        # Sort by SKU for better readability\n",
    "        pivot_table = pivot_table.sort_values(['SKU', 'TITLE'])\n",
    "        pivot_table.to_excel(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\nLatest date analysis saved to: {output_path}\")\n",
    "        print(f\"Rows processed: {len(pivot_table)}\")\n",
    "        print(f\"Unique SKUs: {pivot_table['SKU'].nunique()}\")\n",
    "        \n",
    "        return pivot_table\n",
    "    else:\n",
    "        print(\"Failed to process latest file\")\n",
    "        return None\n",
    "\n",
    "# Main execution with 3 processing options\n",
    "try:\n",
    "    with open('config.json', 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "    folder_path = config_data['paths']['comparision_data_folder']\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading config.json: {e}\")\n",
    "    print(\"Using default DSR folder path...\")\n",
    "    folder_path = \"ComparisionData\"\n",
    "year = input(\"Please enter the year (e.g., 2024): \")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "choice = input(\"\\nChoose processing option:\\n1. Combined file only (all historical data)\\n2. Combined file + Latest date file (all data + separate latest)\\n3. Latest date file only (fastest - processes only newest file)\\nEnter your choice (1, 2, or 3): \").strip()\n",
    "\n",
    "if choice == \"1\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING: COMBINED FILE ONLY\")\n",
    "    print(\"=\"*60)\n",
    "    combined_result = process_folder_and_combine(folder_path, year, create_latest_file=False)\n",
    "    \n",
    "    if combined_result is not None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"File created:\")\n",
    "        print(\"✓ Combined_Price_Analysis.xlsx - All historical data\")\n",
    "        print(f\"\\nTotal rows processed: {len(combined_result)}\")\n",
    "        print(f\"Date range: {combined_result['Date'].min()} to {combined_result['Date'].max()}\")\n",
    "    else:\n",
    "        print(\"Failed to process files\")\n",
    "\n",
    "elif choice == \"2\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING: COMBINED + LATEST FILES\")\n",
    "    print(\"=\"*60)\n",
    "    combined_result = process_folder_and_combine(folder_path, year, create_latest_file=True)\n",
    "    \n",
    "    if combined_result is not None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"Files created:\")\n",
    "        print(\"✓ Combined_Price_Analysis.xlsx - All historical data\")\n",
    "        print(\"✓ Latest_Date_Analysis_[DATE].xlsx - Latest date data only\")\n",
    "        print(f\"\\nTotal rows processed: {len(combined_result)}\")\n",
    "        print(f\"Date range: {combined_result['Date'].min()} to {combined_result['Date'].max()}\")\n",
    "    else:\n",
    "        print(\"Failed to process files\")\n",
    "\n",
    "elif choice == \"3\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING: LATEST DATE FILE ONLY\")\n",
    "    print(\"=\"*60)\n",
    "    latest_result = process_latest_file_only(folder_path, year)\n",
    "    \n",
    "    if latest_result is not None:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"File created:\")\n",
    "        print(\"✓ Latest_Date_Analysis_[DATE].xlsx - Latest date data only\")\n",
    "        print(f\"\\nRows processed: {len(latest_result)}\")\n",
    "        print(\"\\nFirst 5 rows of latest data:\")\n",
    "        print(latest_result.head())\n",
    "    else:\n",
    "        print(\"Failed to process latest file\")\n",
    "\n",
    "else:\n",
    "    print(\"Invalid choice. Please run again and select 1, 2, or 3.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
